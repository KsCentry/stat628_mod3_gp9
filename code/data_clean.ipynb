{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight data preprocession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression\n",
    "\n",
    "Run decompression.bat, csv will be stored in ../code/raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T01:17:09.372306Z",
     "start_time": "2024-10-28T01:17:08.585380Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pytz\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T01:17:47.125948Z",
     "start_time": "2024-10-28T01:17:43.337864Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../data/cleaned_data/flights_data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('../data/cleaned_data/flights_data.parquet', engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for csv in os.listdir('../data/raw_data/'):\n",
    "    data_temp = pd.read_csv('../data/raw_data/' + csv)\n",
    "    data = pd.concat([data, data_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop useless  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLIGHTS all equal to 1\n",
    "data = data.drop('FLIGHTS', axis=1)\n",
    "# DUP all equal to N\n",
    "data = data.drop('DUP', axis=1)\n",
    "# same num of unique values in ORIGIN_AIRPORT_ID and ORIGIN, keep ORIGIN\n",
    "assert data['ORIGIN_AIRPORT_ID'].unique().__len__() == data['ORIGIN'].unique().__len__()\n",
    "data = data.drop('ORIGIN_AIRPORT_ID', axis=1)\n",
    "# same reason for dropping DEST_AIRPORT_ID\n",
    "assert data['DEST_AIRPORT_ID'].unique().__len__() == data['DEST'].unique().__len__()\n",
    "data = data.drop('DEST_AIRPORT_ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State names are included in city names\n",
    "data = data.drop(['ORIGIN_STATE_NM', 'DEST_STATE_NM'], axis=1)\n",
    "# Elapsed time can be calculated from dep and arr time\n",
    "data = data.drop(['CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME'], axis=1)\n",
    "# Distance group are derived by distance\n",
    "data = data.drop('DISTANCE_GROUP', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get airport timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     IATA Timezone\n",
      "7055  BIH       \\N\n",
      "Missing airports: {'XWA', 'EAR', 'IFP'}\n"
     ]
    }
   ],
   "source": [
    "# same airports in column ORIGIN and DEST\n",
    "assert np.array_equal(np.unique(data['ORIGIN'].unique()), np.unique(data['DEST'].unique()))\n",
    "# get airports timezone\n",
    "airport_list = data['ORIGIN'].unique()\n",
    "missing_airport_info = pd.read_csv('../data/downloaded_data/airports.dat', header=None,\n",
    "                           usecols=[4, 11], names=['IATA', 'Timezone'])\n",
    "# filter airports in ORIGIN\n",
    "airport_list = missing_airport_info[missing_airport_info['IATA'].isin(airport_list)]\n",
    "# find airports missing timezone, search and concat manually\n",
    "missing_airports = set(data['ORIGIN'].unique()) - set(airport_list['IATA'])\n",
    "print(airport_list[airport_list['Timezone']=='\\\\N'])\n",
    "airport_list = airport_list[airport_list['IATA'] != 'BIH']\n",
    "print(\"Missing airports:\", missing_airports)\n",
    "missing_airports = {\n",
    "    'IATA': ['BIH', 'XWA', 'IFP', 'EAR'],\n",
    "    'Timezone': ['America/Los_Angeles', 'America/Chicago', 'America/Phoenix', 'America/Chicago']\n",
    "}\n",
    "missing_airports = pd.DataFrame(missing_airports)\n",
    "airport_list = pd.concat([airport_list, missing_airports], ignore_index=True)\n",
    "airport_list = airport_list.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take 30 mins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FL_DATE'] = pd.to_datetime(data['FL_DATE'], format=\"%m/%d/%Y %I:%M:%S %p\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_column(date_col, time_column):\n",
    "    date_adjusted = date_col.where(~time_column.isna(), pd.NaT)\n",
    "    time_adjusted = time_column.fillna(0).astype(int).astype(str).str.zfill(4)\n",
    "    \n",
    "    date_adjusted = date_adjusted.where(time_adjusted != \"2400\", date_adjusted + pd.Timedelta(days=1))\n",
    "    time_adjusted = time_adjusted.where(time_adjusted != \"2400\", \"0000\")\n",
    "    \n",
    "    return date_adjusted, time_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_dep_date, crs_dep_time_str = process_time_column(data['FL_DATE'], data['CRS_DEP_TIME'])\n",
    "crs_arr_date, crs_arr_time_str = process_time_column(data['FL_DATE'], data['CRS_ARR_TIME'])\n",
    "dep_date, dep_time_str = process_time_column(data['FL_DATE'], data['DEP_TIME'])\n",
    "arr_date, arr_time_str = process_time_column(data['FL_DATE'], data['ARR_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CRS_DEP_TIMESTAMP'] = pd.to_datetime(crs_dep_date.astype(str) + \" \" + crs_dep_time_str.str[:2] + \":\" + crs_dep_time_str.str[2:], errors='coerce')\n",
    "print('>>>')\n",
    "data['CRS_ARR_TIMESTAMP'] = pd.to_datetime(crs_arr_date.astype(str) + \" \" + crs_arr_time_str.str[:2] + \":\" + crs_arr_time_str.str[2:], errors='coerce')\n",
    "print('>>>')\n",
    "data['DEP_TIMESTAMP'] = pd.to_datetime(dep_date.astype(str) + \" \" + dep_time_str.str[:2] + \":\" + dep_time_str.str[2:], errors='coerce')\n",
    "print('>>>')\n",
    "data['ARR_TIMESTAMP'] = pd.to_datetime(arr_date.astype(str) + \" \" + arr_time_str.str[:2] + \":\" + arr_time_str.str[2:], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 MINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_tz = data.merge(airport_list[['IATA', 'Timezone']], left_on='ORIGIN', right_on='IATA', how='left')['Timezone']\n",
    "dest_tz = data.merge(airport_list[['IATA', 'Timezone']], left_on='DEST', right_on='IATA', how='left')['Timezone']\n",
    "\n",
    "def convert_to_utc(timestamp, timezone_str):\n",
    "    local_tz = pytz.timezone(timezone_str)\n",
    "    return local_tz.localize(timestamp).astimezone(pytz.UTC)\n",
    "\n",
    "data['CRS_DEP_UTC'] = [convert_to_utc(ts, tz) for ts, tz in zip(data['CRS_DEP_TIMESTAMP'], origin_tz)]\n",
    "data['CRS_ARR_UTC'] = [convert_to_utc(ts, tz) for ts, tz in zip(data['CRS_ARR_TIMESTAMP'], dest_tz)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FL_DATE', 'OP_UNIQUE_CARRIER', 'OP_CARRIER_FL_NUM', 'ORIGIN',\n",
       "       'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'CRS_DEP_TIME',\n",
       "       'DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME', 'CANCELLED',\n",
       "       'CANCELLATION_CODE', 'DIVERTED', 'AIR_TIME', 'DISTANCE',\n",
       "       'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY',\n",
       "       'LATE_AIRCRAFT_DELAY', 'CRS_DEP_TIMESTAMP', 'CRS_ARR_TIMESTAMP',\n",
       "       'DEP_TIMESTAMP', 'ARR_TIMESTAMP', 'CRS_DEP_UTC', 'CRS_ARR_UTC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop processed time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['FL_DATE', 'CRS_DEP_TIME', 'DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference webpage:\n",
    "\n",
    "Iowa Environment Mesonet, ASOS data download.\n",
    "https://mesonet.agron.iastate.edu/request/download.phtml?network=IA_ASOS\n",
    "\n",
    "Github repo\n",
    "https://github.com/akrherz/iem/blob/main/pylib/iemweb/request/asos.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T01:40:25.361344Z",
     "start_time": "2024-10-28T01:40:24.858337Z"
    }
   },
   "outputs": [],
   "source": [
    "weather = pd.read_parquet('../data/cleaned_data/weather.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_parquet('../data/cleaned_data/weather.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By IATA code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_fields = ['tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'vsby', 'gust', 'alti', 'mslp', 'wxcodes']\n",
    "essential_fields = ''.join([f\"&data={field}\" for field in essential_fields])\n",
    "\n",
    "station_names = airport_list['IATA'].values\n",
    "\n",
    "all_data = []\n",
    "df = pd.DataFrame()\n",
    "no_data_station = []\n",
    "iteration = 1\n",
    "years = range(2018, 2025)\n",
    "months = [1, 11, 12]\n",
    "current_year = datetime.now().year\n",
    "\n",
    "for station_name in station_names:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            if year == current_year and month in [11, 12]:\n",
    "                continue\n",
    "            if month == 1:\n",
    "                year2 = year\n",
    "                month2 = 2\n",
    "                day1, day2 = 1, 1\n",
    "            elif month == 11:\n",
    "                year2 = year\n",
    "                month2 = 12\n",
    "                day1, day2 = 1, 1\n",
    "            else:\n",
    "                year2 = year + 1\n",
    "                month2 = 1\n",
    "                day1, day2 = 1, 1\n",
    "            uri = (\n",
    "                \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "                f\"station={station_name}\" + essential_fields +\n",
    "                f\"&year1={year}&month1={month}&day1={day1}\"\n",
    "                f\"&year2={year2}&month2={month2}&day2={day2}\"\n",
    "                \"&tz=Etc/UTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T\"\n",
    "                \"&direct=yes&report_type=3\"\n",
    "            )\n",
    "            try:\n",
    "                res = requests.get(uri)\n",
    "                res.raise_for_status()\n",
    "                df = pd.read_csv(io.StringIO(res.text), na_values='M')\n",
    "                df['valid'] = pd.to_datetime(df['valid'], format='%Y-%m-%d %H:%M')\n",
    "                if df.empty:\n",
    "                    no_data_station.append(station_name)\n",
    "                    continue\n",
    "                else:\n",
    "                    all_data.append(df)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Failed to retrieve data for {station_name} in {year}-{month}: {e}\")\n",
    "    print(iteration, station_name, len(df))\n",
    "    iteration += 1\n",
    "\n",
    "weather = pd.concat(all_data, ignore_index=True)\n",
    "# Substitute trace precipitation as 0.0001\n",
    "weather['p01i'] = weather['p01i'].replace('T', 0.0001).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By ICAO code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot get weather data for some airports, try to use ICAO code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find airports with no data\n",
    "missing_airports = set(airport_list['IATA'].values) - set(weather['station'].unique())\n",
    "print(missing_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_info = pd.read_csv('../data/downloaded_data/airports.dat', header=None, usecols=[4, 5, 6, 7, 8],\n",
    "                           names=[\"IATA\", \"ICAO\", \"Latitude\", \"Longitude\", \"Altitude\"])\n",
    "missing_airport_info = airport_info[airport_info['IATA'].isin(missing_airports)]\n",
    "missing_airports = missing_airport_info['ICAO'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "no_data_station = []\n",
    "iteration = 1\n",
    "years = range(2018, 2025)\n",
    "months = [1, 11, 12]\n",
    "current_year = datetime.now().year\n",
    "\n",
    "for station_name in missing_airports:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            if year == current_year and month in [11, 12]:\n",
    "                continue\n",
    "            if month == 1:\n",
    "                year2 = year\n",
    "                month2 = 2\n",
    "                day1, day2 = 1, 1\n",
    "            elif month == 11:\n",
    "                year2 = year\n",
    "                month2 = 12\n",
    "                day1, day2 = 1, 1\n",
    "            else:\n",
    "                year2 = year + 1\n",
    "                month2 = 1\n",
    "                day1, day2 = 1, 1\n",
    "            uri = (\n",
    "                \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "                f\"station={station_name}\" + essential_fields +\n",
    "                f\"&year1={year}&month1={month}&day1={day1}\"\n",
    "                f\"&year2={year2}&month2={month2}&day2={day2}\"\n",
    "                \"&tz=Etc/UTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T\"\n",
    "                \"&direct=yes&report_type=3\"\n",
    "            )\n",
    "            try:\n",
    "                res = requests.get(uri)\n",
    "                res.raise_for_status()\n",
    "                df = pd.read_csv(io.StringIO(res.text), na_values='M')\n",
    "                df['valid'] = pd.to_datetime(df['valid'], format='%Y-%m-%d %H:%M')\n",
    "                if df.empty:\n",
    "                    no_data_station.append(station_name)\n",
    "                    continue\n",
    "                else:\n",
    "                    all_data.append(df)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Failed to retrieve data for {station_name} in {year}-{month}: {e}\")\n",
    "    print(iteration, station_name, len(df))\n",
    "    iteration += 1\n",
    "\n",
    "weather_missing = pd.concat(all_data, ignore_index=True)\n",
    "weather_missing['p01i'] = weather_missing['p01i'].replace('T', 0.0001).astype(float)\n",
    "weather = pd.concat([weather, weather_missing], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repalce ICAO code to IATA code\n",
    "icao_to_iata = airport_info[airport_info['IATA'].isin(airport_list['IATA'])].set_index('ICAO')['IATA'].to_dict()\n",
    "weather['station'] = weather['station'].replace(icao_to_iata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have no data of Kapaluaâ€“West Maui Airport, which is a regional airport in the district of Mahinahina on the west side of Maui island in the state of Hawaii. There is no data of this airport on METAR. Based on the difficulty of getting this data, we decide to remove observations with airport JHM, for ORIGIN and DEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['ORIGIN'] != 'JHM') & (data['DEST'] != 'JHM')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge flight and weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weather data are missing after UTC time conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "date_range = [pd.Timestamp(\"2017-12-31\")]\n",
    "for year in range(2018, 2024):\n",
    "    date_range.append(pd.Timestamp(f\"{year}-10-31\"))\n",
    "essential_fields = ['tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'vsby', 'gust', 'alti', 'mslp', 'wxcodes']\n",
    "essential_fields = ''.join([f\"&data={field}\" for field in essential_fields])\n",
    "for station_name in airport_list['IATA']:\n",
    "    for date in date_range:\n",
    "        if date.year == 2017:\n",
    "            year2 = date.year + 1\n",
    "        else:\n",
    "            year2 = date.year\n",
    "        uri = (\n",
    "            \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "            f\"station={station_name}\" + essential_fields +\n",
    "            f\"&year1={date.year}&month1={date.month}&day1=31\"\n",
    "            f\"&year2={year2}&month2={(date.month + 1) % 12}&day2=1\"\n",
    "            \"&tz=Etc/UTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T\"\n",
    "            \"&direct=yes&report_type=3\"\n",
    "        )\n",
    "        res = requests.get(uri)\n",
    "        df = pd.read_csv(io.StringIO(res.text), na_values='M')\n",
    "        df['valid'] = pd.to_datetime(df['valid'], format='%Y-%m-%d %H:%M')\n",
    "        all_data.append(df)\n",
    "\n",
    "weather_missing = pd.concat(all_data, ignore_index=True)\n",
    "weather_missing['p01i'] = weather_missing['p01i'].replace('T', 0.0001).astype(float)\n",
    "weather_missing['valid'] = weather_missing['valid'].dt.tz_localize('UTC')\n",
    "weather = pd.concat([weather, weather_missing], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_24668\\3577489395.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  weather_missing = pd.concat(all_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "date_range = []\n",
    "for year in range(2018, 2025):\n",
    "    date_range.append(pd.Timestamp(f\"{year}-02-01\"))\n",
    "essential_fields = ['tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'vsby', 'gust', 'alti', 'mslp', 'wxcodes']\n",
    "essential_fields = ''.join([f\"&data={field}\" for field in essential_fields])\n",
    "for station_name in airport_list['IATA']:\n",
    "    for date in date_range:\n",
    "        uri = (\n",
    "            \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "            f\"station={station_name}\" + essential_fields +\n",
    "            f\"&year1={date.year}&month1=2&day1=1\"\n",
    "            f\"&year2={date.year}&month2=2&day2=2\"\n",
    "            \"&tz=Etc/UTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T\"\n",
    "            \"&direct=yes&report_type=3\"\n",
    "        )\n",
    "        res = requests.get(uri)\n",
    "        df = pd.read_csv(io.StringIO(res.text), na_values='M')\n",
    "        df['valid'] = pd.to_datetime(df['valid'], format='%Y-%m-%d %H:%M')\n",
    "        all_data.append(df)\n",
    "\n",
    "weather_missing = pd.concat(all_data, ignore_index=True)\n",
    "weather_missing['p01i'] = weather_missing['p01i'].replace('T', 0.0001).astype(float)\n",
    "weather_missing['valid'] = weather_missing['valid'].dt.tz_localize('UTC')\n",
    "weather = pd.concat([weather, weather_missing], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         OP_UNIQUE_CARRIER  OP_CARRIER_FL_NUM ORIGIN ORIGIN_CITY_NAME DEST  \\\n",
      "0                       9K               6393    GUM         Guam, TT  SPN   \n",
      "1                       9K               6394    SPN       Saipan, TT  GUM   \n",
      "2                       9K               6395    GUM         Guam, TT  SPN   \n",
      "18030                   UA                200    GUM         Guam, TT  HNL   \n",
      "3                       9K               6392    SPN       Saipan, TT  GUM   \n",
      "...                    ...                ...    ...              ...  ...   \n",
      "10924307                DL                485    KOA         Kona, HI  SEA   \n",
      "10924295                DL                975    LIH        Lihue, HI  SEA   \n",
      "10923676                UA               1639    OGG      Kahului, HI  SFO   \n",
      "10923933                UA               1724    KOA         Kona, HI  SFO   \n",
      "10924706                AS                848    HNL     Honolulu, HI  SEA   \n",
      "\n",
      "             DEST_CITY_NAME  CANCELLED CANCELLATION_CODE  DIVERTED  AIR_TIME  \\\n",
      "0                Saipan, TT        0.0              None       0.0      40.0   \n",
      "1                  Guam, TT        0.0              None       0.0      40.0   \n",
      "2                Saipan, TT        0.0              None       0.0      35.0   \n",
      "18030          Honolulu, HI        0.0              None       0.0     395.0   \n",
      "3                  Guam, TT        0.0              None       0.0      40.0   \n",
      "...                     ...        ...               ...       ...       ...   \n",
      "10924307        Seattle, WA        0.0              None       0.0     297.0   \n",
      "10924295        Seattle, WA        0.0              None       0.0     286.0   \n",
      "10923676  San Francisco, CA        0.0              None       0.0     245.0   \n",
      "10923933  San Francisco, CA        0.0              None       0.0     249.0   \n",
      "10924706        Seattle, WA        0.0              None       0.0     292.0   \n",
      "\n",
      "          ...  dwpf_arr  relh_arr  drct_arr  sknt_arr  p01i_arr  vsby_arr  \\\n",
      "0         ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "1         ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2         ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "18030     ...      60.0     53.95      50.0       9.0    0.0000      10.0   \n",
      "3         ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "...       ...       ...       ...       ...       ...       ...       ...   \n",
      "10924307  ...      46.0     74.26     120.0      13.0    0.0100      10.0   \n",
      "10924295  ...      46.0     74.26     120.0      13.0    0.0100      10.0   \n",
      "10923676  ...      48.0     66.83     120.0      11.0    0.0000      10.0   \n",
      "10923933  ...      49.0     66.95     170.0      18.0    0.0001      10.0   \n",
      "10924706  ...      45.0     68.93     110.0      13.0    0.0100      10.0   \n",
      "\n",
      "         gust_arr alti_arr mslp_arr wxcodes_arr  \n",
      "0             NaN      NaN      NaN         NaN  \n",
      "1             NaN      NaN      NaN         NaN  \n",
      "2             NaN      NaN      NaN         NaN  \n",
      "18030         NaN    30.05   1017.5        None  \n",
      "3             NaN      NaN      NaN         NaN  \n",
      "...           ...      ...      ...         ...  \n",
      "10924307     22.0    29.48    998.9         -RA  \n",
      "10924295     22.0    29.48    998.9         -RA  \n",
      "10923676      NaN    29.79   1008.8        None  \n",
      "10923933     28.0    29.78   1008.3        None  \n",
      "10924706     22.0    29.48    999.0        None  \n",
      "\n",
      "[10939582 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "data = data.sort_values('CRS_DEP_UTC')\n",
    "weather = weather.sort_values('valid')\n",
    "\n",
    "data_with_dep_weather = pd.merge_asof(\n",
    "    data,\n",
    "    weather,\n",
    "    left_on='CRS_DEP_UTC',\n",
    "    right_on='valid',\n",
    "    left_by='ORIGIN',\n",
    "    right_by='station',\n",
    "    suffixes=('', '_dep')\n",
    ")\n",
    "\n",
    "data_with_dep_weather = data_with_dep_weather.sort_values('CRS_ARR_UTC')\n",
    "\n",
    "data_with_all_weather = pd.merge_asof(\n",
    "    data_with_dep_weather,\n",
    "    weather,\n",
    "    left_on='CRS_ARR_UTC',\n",
    "    right_on='valid',\n",
    "    left_by='DEST',\n",
    "    right_by='station',\n",
    "    suffixes=('', '_arr')\n",
    ")\n",
    "\n",
    "data_with_all_weather = data_with_all_weather.sort_values('CRS_DEP_UTC')\n",
    "print(data_with_all_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_all_weather = data_with_all_weather.drop(['station', 'valid', 'station_arr', 'valid_arr'], axis=1)\n",
    "# drop column gust because so many missing values(87%)\n",
    "data_with_all_weather = data_with_all_weather.drop(['gust', 'gust_arr'], axis=1)\n",
    "data_with_all_weather = data_with_all_weather.sort_values('CRS_DEP_TIMESTAMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_all_weather.to_parquet('../data/cleaned_data/data_with_all_weather.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_all_weather = pd.read_parquet('../data/cleaned_data/data_with_all_weather.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
